% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/xgb_opt.R
\name{xgb_opt}
\alias{xgb_opt}
\title{Bayesian optimization for xgboost}
\usage{
xgb_opt(train_data, train_label, test_data, test_label, objectfun, evalmetric,
  init_points = 20, n_iter = 1, acq = "ei", kappa = 2.576, eps = 0,
  classes = NULL)
}
\arguments{
\item{train_data}{A data frame for training of xgboost}

\item{train_label}{The column of class to classify in the training data}

\item{test_data}{A data frame for training of xgboost}

\item{test_label}{The column of class to classify in the test data}

\item{objectfun}{Specify the learning task and the corresponding learning objective
\itemize{
    \item \code{reg:linear} linear regression (Default).
    \item \code{reg:logistic} logistic regression.
    \item \code{binary:logistic} logistic regression for binary classification. Output probability.
    \item \code{binary:logitraw} logistic regression for binary classification, output score before logistic transformation.
    \item \code{num_class} set the number of classes. To use only with multiclass objectives.
    \item \code{multi:softmax} set xgboost to do multiclass classification using the softmax objective. Class is represented by a number and should be from 0 to \code{num_class - 1}.
    \item \code{multi:softprob} same as softmax, but prediction outputs a vector of ndata * nclass elements, which can be further reshaped to ndata, nclass matrix. The result contains predicted probabilities of each data point belonging to each class.
    \item \code{rank:pairwise} set xgboost to do ranking task by minimizing the pairwise loss.
  }}

\item{evalmetric}{evaluation metrics for validation data. Users can pass a self-defined function to it. Default: metric will be assigned according to objective(rmse for regression, and error for classification, mean average precision for ranking). List is provided in detail section.}

\item{init_points}{Number of randomly chosen points to sample the
target function before Bayesian Optimization fitting the Gaussian Process.}

\item{n_iter}{Total number of times the Bayesian Optimization is to repeated.}

\item{acq}{Acquisition function type to be used. Can be "ucb", "ei" or "poi".
\itemize{
  \item \code{ucb} GP Upper Confidence Bound
  \item \code{ei} Expected Improvement
  \item \code{poi} Probability of Improvement
}}

\item{kappa}{tunable parameter kappa of GP Upper Confidence Bound, to balance exploitation against exploration,
increasing kappa will make the optimized hyperparameters pursuing exploration.}

\item{eps}{tunable parameter epsilon of Expected Improvement and Probability of Improvement, to balance exploitation against exploration,
increasing epsilon will make the optimized hyperparameters are more spread out across the whole range.}

\item{classes}{set the number of classes. To use only with multiclass objectives.}
}
\value{
Best parameters for xgboost.
}
\description{
This function estimates parameters for xgboost based on bayesian optimization.
}

