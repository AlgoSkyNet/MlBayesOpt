<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>MlBayesOpt • MlBayesOpt</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">MlBayesOpt</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/MlBayesOpt.html">Get Started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/ymattu/MlBayesOpt">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>MlBayesOpt</h1>
                        <h4 class="author">Yuya Matsumura</h4>
            
            <h4 class="date">2017-10-18</h4>
          </div>

    
    
<div class="contents">
<div id="overview" class="section level2">
<h2 class="hasAnchor">
<a href="#overview" class="anchor"></a>Overview</h2>
<p>This is an R package to tune hyperparameters for machine learning algorithms using Bayesian Optimization based on Gaussian Processes. Algorithms currently supported are: Support vector machines, Random forest, and XGboost.</p>
<p>This package has some features:</p>
<ul>
<li>It’s very easy to write Bayesian Optimaization function, but you also able to customise your model very easily.</li>
<li>Tidy.</li>
</ul>
<div id="hyperprameter-tuning-in-machine-learning" class="section level3">
<h3 class="hasAnchor">
<a href="#hyperprameter-tuning-in-machine-learning" class="anchor"></a>Hyperprameter Tuning in Machine Learning</h3>
<p>In many methods of machinelearning, it is very important to tune hyperparameters. “Grid Search” was often used to search the appropriate hyperaprameters, but it takes too much time to compute.</p>
<p>To solve this problem, Bayesian Optimization is often used to tune hyperparameters fast. This is a sequential design strategy for global optimization of black-box functions.</p>
<p>While Grid Search is simply an exhaustive searching through a manually specified subset of the hyperparameter space, Bayesian Optimization constructs a posterior distribution of functions (gaussian process) that describes the function you want to optimize best, and search the point whose score may be better.</p>
</div>
<div id="machine-learning-and-bayesian-optimization-in-r" class="section level3">
<h3 class="hasAnchor">
<a href="#machine-learning-and-bayesian-optimization-in-r" class="anchor"></a>Machine Learning and Bayesian Optimization in R</h3>
<p>We could execute bayesian optimization using <code>rBayesianOptimization</code> package in the past.</p>
<ol start="0" style="list-style-type: decimal">
<li>Make data</li>
<li>Make the function to maximize</li>
<li>Execute the Bayesian Optimization</li>
</ol>
<p>For example, if you want to tune hyperparameters of XGboost with 5-fold cross validation, you have to write as following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(xgboost)
<span class="kw">library</span>(Matrix)

<span class="kw">data</span>(agaricus.train, <span class="dt">package =</span> <span class="st">"xgboost"</span>)
dtrain &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/xgboost/topics/xgb.DMatrix">xgb.DMatrix</a></span>(agaricus.train<span class="op">$</span>data,
                      <span class="dt">label =</span> agaricus.train<span class="op">$</span>label)
cv_folds &lt;-<span class="st"> </span><span class="kw">KFold</span>(agaricus.train<span class="op">$</span>label, <span class="dt">nfolds =</span> <span class="dv">5</span>,
                  <span class="dt">stratified =</span> <span class="ot">TRUE</span>, <span class="dt">seed =</span> <span class="dv">0</span>)
xgb_cv_bayes &lt;-<span class="st"> </span><span class="cf">function</span>(max_depth, min_child_weight, subsample) {
  cv &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/xgboost/topics/xgb.cv">xgb.cv</a></span>(<span class="dt">params =</span> <span class="kw">list</span>(<span class="dt">booster =</span> <span class="st">"gbtree"</span>, <span class="dt">eta =</span> <span class="fl">0.01</span>,
                             <span class="dt">max_depth =</span> max_depth,
                             <span class="dt">min_child_weight =</span> min_child_weight,
                             <span class="dt">subsample =</span> subsample, <span class="dt">colsample_bytree =</span> <span class="fl">0.3</span>,
                             <span class="dt">lambda =</span> <span class="dv">1</span>, <span class="dt">alpha =</span> <span class="dv">0</span>,
                             <span class="dt">objective =</span> <span class="st">"binary:logistic"</span>,
                             <span class="dt">eval_metric =</span> <span class="st">"auc"</span>),
               <span class="dt">data =</span> dtrain, <span class="dt">nround =</span> <span class="dv">100</span>,
               <span class="dt">folds =</span> cv_folds, <span class="dt">prediction =</span> <span class="ot">TRUE</span>, <span class="dt">showsd =</span> <span class="ot">TRUE</span>,
               <span class="dt">early_stopping_rounds =</span> <span class="dv">5</span>, <span class="dt">maximize =</span> <span class="ot">TRUE</span>, <span class="dt">verbose =</span> <span class="dv">0</span>)
  <span class="kw">list</span>(<span class="dt">Score =</span> cv<span class="op">$</span>evaluation_log<span class="op">$</span>test_auc_mean[cv<span class="op">$</span>best_iteration],
       <span class="dt">Pred =</span> cv<span class="op">$</span>pred)
}
OPT_Res &lt;-<span class="st"> </span><span class="kw">BayesianOptimization</span>(xgb_cv_bayes,
                                <span class="dt">bounds =</span> <span class="kw">list</span>(<span class="dt">max.depth =</span> <span class="kw">c</span>(2L, 6L),
                                              <span class="dt">min_child_weight =</span> <span class="kw">c</span>(1L, 10L),
                                              <span class="dt">subsample =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.8</span>)),
                                <span class="dt">init_grid_dt =</span> <span class="ot">NULL</span>, <span class="dt">init_points =</span> <span class="dv">10</span>, <span class="dt">n_iter =</span> <span class="dv">20</span>,
                                <span class="dt">acq =</span> <span class="st">"ucb"</span>, <span class="dt">kappa =</span> <span class="fl">2.576</span>, <span class="dt">eps =</span> <span class="fl">0.0</span>,
                                <span class="dt">verbose =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>On the other hand, we can write this very easily with <code>MlBayesOpt</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MlBayesOpt)

res0 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/xgb_cv_opt.html">xgb_cv_opt</a></span>(<span class="dt">data =</span> agaricus.train<span class="op">$</span>data,
                   <span class="dt">label =</span> agaricus.train<span class="op">$</span>label,
                   <span class="dt">objectfun =</span> <span class="st">"binary:logistic"</span>,
                   <span class="dt">evalmetric =</span> <span class="st">"auc"</span>,
                   <span class="dt">n_folds =</span> <span class="dv">5</span>,
                   <span class="dt">acq =</span> <span class="st">"ucb"</span>,
                   <span class="dt">init_points =</span> <span class="dv">10</span>,
                   <span class="dt">n_iter =</span> <span class="dv">20</span>)</code></pre></div>
<p>When the data has <code>data.frame</code> class, you have only to write column name to specify the label.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># This takes a lot of time</span>
<span class="co"># fashion data is included in this package</span>
res0 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/xgb_cv_opt.html">xgb_cv_opt</a></span>(<span class="dt">data =</span> fashion,
                   <span class="dt">label =</span> y,
                   <span class="dt">objectfun =</span> <span class="st">"multi:softmax"</span>,
                   <span class="dt">evalmetric =</span> <span class="st">"merror"</span>,
                   <span class="dt">n_folds =</span> <span class="dv">15</span>,
                   <span class="dt">classes =</span> <span class="dv">10</span>)</code></pre></div>
</div>
</div>
<div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation</h2>
<p>You can install <strong>MlBayesOpt</strong> from CRAN:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">"MlbayesOpt"</span>)</code></pre></div>
<p>You can install MlBayesOpt (latest dev version) from github with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># install.packages("githubinstall")</span>
githubinstall<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/githubinstall/topics/gh_install_packages">githubinstall</a></span>(<span class="st">"MlBayesOpt"</span>)

<span class="co"># install.packages("devtools")</span>
devtools<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/devtools/topics/install_github">install_github</a></span>(<span class="st">"ymattu/MlBayesOpt"</span>)</code></pre></div>
<p>The source code for <strong>MlBayesOpt</strong> package is available on GitHub at</p>
<ul>
<li><a href="https://github.com/ymattu/MlBayesOpt" class="uri">https://github.com/ymattu/MlBayesOpt</a></li>
</ul>
</div>
<div id="details" class="section level2">
<h2 class="hasAnchor">
<a href="#details" class="anchor"></a>Details</h2>
<div id="support-vector-machine" class="section level3">
<h3 class="hasAnchor">
<a href="#support-vector-machine" class="anchor"></a>Support Vector Machine</h3>
<p>About SVM, this package supports hold-out tuning (<code><a href="../reference/svm_opt.html">svm_opt()</a></code>) and cross-validation tuning(<code><a href="../reference/svm_cv_opt.html">svm_cv_opt()</a></code>).</p>
</div>
<div id="random-forest" class="section level3">
<h3 class="hasAnchor">
<a href="#random-forest" class="anchor"></a>Random Forest</h3>
<p>This package supports only hold-out tuning so far about Random Forest(<code><a href="../reference/rf_opt.html">rf_opt()</a></code>).</p>
</div>
<div id="xgboost" class="section level3">
<h3 class="hasAnchor">
<a href="#xgboost" class="anchor"></a>XGboost</h3>
<p>For XGboost, this package supports hold-out tuning (<code><a href="../reference/xgb_opt.html">xgb_opt()</a></code>) and cross-validation tuning(<code><a href="../reference/xgb_cv_opt.html">xgb_cv_opt()</a></code>).</p>
</div>
</div>
<div id="options-for-bayesian-optimization" class="section level2">
<h2 class="hasAnchor">
<a href="#options-for-bayesian-optimization" class="anchor"></a>Options for Bayesian Optimization</h2>
</div>
<div id="related-works" class="section level2">
<h2 class="hasAnchor">
<a href="#related-works" class="anchor"></a>Related Works</h2>
<ul>
<li><a href="https://github.com/yanyachen/rBayesianOptimization">rBayesianOptimization</a></li>
<li><a href="https://cran.r-project.org/web/packages/e1071/index.html">e1071</a></li>
<li><a href="https://github.com/imbs-hl/ranger">ranger</a></li>
<li><a href="https://github.com/dmlc/xgboost">xgboost</a></li>
</ul>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#overview">Overview</a></li>
      <li><a href="#installation">Installation</a></li>
      <li><a href="#details">Details</a></li>
      <li><a href="#options-for-bayesian-optimization">Options for Bayesian Optimization</a></li>
      <li><a href="#related-works">Related Works</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Yuya Matsumura.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
